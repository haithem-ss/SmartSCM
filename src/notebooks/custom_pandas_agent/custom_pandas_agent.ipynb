{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c506dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\",\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07442c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents import create_pandas_dataframe_agent as create_default_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREFIX = \"\"\"\n",
    "You are working with a pandas dataframe in Python. The name of the dataframe is `df`.\n",
    "You should use the tools below to answer the question posed of you:\"\"\"\n",
    "\n",
    "SUFFIX_NO_DF = \"\"\"\n",
    "Begin!\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "SUFFIX_WITH_DF = \"\"\"\n",
    "Here is the documentation for the dataframe:\n",
    "{df_doc}\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Literal, Optional, Sequence, Union, cast\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.agents import (\n",
    "    AgentType,\n",
    "    create_react_agent,\n",
    ")\n",
    "from langchain.agents.agent import (\n",
    "    AgentExecutor,\n",
    "    BaseMultiActionAgent,\n",
    "    BaseSingleActionAgent,\n",
    "    RunnableAgent,\n",
    ")\n",
    "from langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackManager\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.prompts import (\n",
    "    BasePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.interactive_env import is_interactive_env\n",
    "\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "\n",
    "from tools.rag_tool import RAGTool\n",
    "\n",
    "\n",
    "def _get_single_prompt(\n",
    "    df: Any,\n",
    "    *,\n",
    "    prefix: Optional[str] = None,\n",
    "    suffix: Optional[str] = None,\n",
    "    include_df_in_prompt: Optional[bool] = True,\n",
    "    number_of_head_rows: int = 5,\n",
    ") -> BasePromptTemplate:\n",
    "    if suffix is not None:\n",
    "        suffix_to_use = suffix\n",
    "    elif include_df_in_prompt:\n",
    "        suffix_to_use = SUFFIX_WITH_DF\n",
    "    else:\n",
    "        suffix_to_use = SUFFIX_NO_DF\n",
    "    prefix = prefix if prefix is not None else PREFIX\n",
    "\n",
    "    template = \"\\n\\n\".join([prefix, \"{tools}\", FORMAT_INSTRUCTIONS, suffix_to_use])\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    partial_prompt = prompt.partial()\n",
    "\n",
    "    return partial_prompt\n",
    "\n",
    "\n",
    "def _get_prompt(df: Any, **kwargs: Any) -> BasePromptTemplate:\n",
    "    return _get_single_prompt(df, **kwargs)\n",
    "\n",
    "\n",
    "def create_pandas_dataframe_agent(\n",
    "    llm: LanguageModelLike,\n",
    "    df: Any,\n",
    "    agent_type: Union[\n",
    "        AgentType, Literal[\"openai-tools\", \"tool-calling\"]\n",
    "    ] = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    callback_manager: Optional[BaseCallbackManager] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    suffix: Optional[str] = None,\n",
    "    input_variables: Optional[List[str]] = None,\n",
    "    verbose: bool = False,\n",
    "    return_intermediate_steps: bool = False,\n",
    "    max_iterations: Optional[int] = 15,\n",
    "    max_execution_time: Optional[float] = None,\n",
    "    early_stopping_method: str = \"force\",\n",
    "    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    include_df_in_prompt: Optional[bool] = True,\n",
    "    number_of_head_rows: int = 5,\n",
    "    extra_tools: Sequence[BaseTool] = (),\n",
    "    engine: Literal[\"pandas\", \"modin\"] = \"pandas\",\n",
    "    allow_dangerous_code: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> AgentExecutor:\n",
    "    if not allow_dangerous_code:\n",
    "        raise ValueError(\n",
    "            \"This agent relies on access to a python repl tool which can execute \"\n",
    "            \"arbitrary code. This can be dangerous and requires a specially sandboxed \"\n",
    "            \"environment to be safely used. Please read the security notice in the \"\n",
    "            \"doc-string of this function. You must opt-in to use this functionality \"\n",
    "            \"by setting allow_dangerous_code=True.\"\n",
    "            \"For general security guidelines, please see: \"\n",
    "            \"https://python.langchain.com/docs/security/\"\n",
    "        )\n",
    "    if is_interactive_env():\n",
    "        pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(f\"Expected pandas DataFrame, got {type(df)}\")\n",
    "\n",
    "    if input_variables:\n",
    "        kwargs = kwargs or {}\n",
    "        kwargs[\"input_variables\"] = input_variables\n",
    "    if kwargs:\n",
    "        warnings.warn(\n",
    "            f\"Received additional kwargs {kwargs} which are no longer supported.\"\n",
    "        )\n",
    "\n",
    "    df_locals = {}\n",
    "\n",
    "    df_locals[\"df\"] = df\n",
    "    tools = [PythonAstREPLTool(locals=df_locals),RAGTool()]\n",
    "\n",
    "    if include_df_in_prompt is not None and suffix is not None:\n",
    "        raise ValueError(\"If suffix is specified, include_df_in_prompt should not be.\")\n",
    "    prompt = _get_prompt(\n",
    "        df,\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "        include_df_in_prompt=include_df_in_prompt,\n",
    "        number_of_head_rows=number_of_head_rows,\n",
    "    )\n",
    "\n",
    "    agent: Union[BaseSingleActionAgent, BaseMultiActionAgent] = RunnableAgent(\n",
    "        runnable=create_react_agent(llm, tools, prompt),  # type: ignore\n",
    "        input_keys_arg=[\"input\"],\n",
    "        return_keys_arg=[\"output\"],\n",
    "    )\n",
    "    return AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=verbose,\n",
    "        return_intermediate_steps=return_intermediate_steps,\n",
    "        max_iterations=max_iterations,\n",
    "        max_execution_time=max_execution_time,\n",
    "        early_stopping_method=early_stopping_method,\n",
    "        **(agent_executor_kwargs or {}),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0f88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "import pandas as pd\n",
    "from langchain_openai.chat_models.base import BaseChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14fae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = BaseChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    openai_api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    openai_api_base=\"https://api.deepseek.com\",\n",
    "    max_tokens=8192,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f298e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.df=None\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    def set_df(self, df):\n",
    "        self.df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf1e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataLoadingTool:\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(\"../../final_data/data/2024-12-01.csv\", sep=\",\")\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbae10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestDataLoadingTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d749d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.get_dataframe()[\"VendorName\"].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05f7200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set pandas display.max_columns to None for interactive env\n",
      "Checking type of dataframe: <class 'pandas.core.frame.DataFrame'>\n",
      "Added df to df_locals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haithem/Documents/PFE/Proof_of_Concept/src/tools/rag_tool.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.data_embeddings = HuggingFaceEmbeddings(model_name=self.model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called _get_single_prompt\n",
      "Single prompt template:\n",
      "\n",
      "You are working with a pandas dataframe in Python. The name of the dataframe is `df`.\n",
      "You should use the tools below to answer the question posed of you:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "\n",
      "Begin!\n",
      "Question: {input}\n",
      "{agent_scratchpad}\n",
      "Partial prompt input variables before filling: ['agent_scratchpad', 'input', 'tool_names', 'tools']\n",
      "Prompt for ZERO_SHOT_REACT_DESCRIPTION: input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] input_types={} partial_variables={} template='\\nYou are working with a pandas dataframe in Python. The name of the dataframe is `df`.\\nYou should use the tools below to answer the question posed of you:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\n\\nBegin!\\nQuestion: {input}\\n{agent_scratchpad}'\n",
      "Created RunnableAgent\n",
      "Returning AgentExecutor\n"
     ]
    }
   ],
   "source": [
    "default_agent = create_default_agent(\n",
    "    llm=llm,\n",
    "    df=test.get_dataframe(),\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    allow_dangerous_code=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "custom_agent = create_pandas_dataframe_agent(\n",
    "    llm=llm,\n",
    "    df=test.get_dataframe(),\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    allow_dangerous_code=True,\n",
    "    verbose=True,\n",
    "    include_df_in_prompt=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "# class LoggingHandler(BaseCallbackHandler):\n",
    "#     def __init__(self):\n",
    "#         self.logs = []\n",
    "\n",
    "#     def on_agent_action(self, action, **kwargs):\n",
    "#         self.logs.append(f\"[Thought+Action]\\n{action.log}\\n\")\n",
    "\n",
    "#     def on_tool_end(self, output, **kwargs):\n",
    "#         self.logs.append(f\"[Tool Output]\\n{output}\\n\")\n",
    "\n",
    "#     def on_agent_finish(self, finish, **kwargs):\n",
    "#         self.logs.append(f\"[Final Answer]\\n{finish.get('output', '')}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b3e5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def benchmark_agent(prompts, filename, agent, K=10):\n",
    "    logs_df = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        for i in range(K):\n",
    "            handler = LoggingHandler()\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                response = agent.run(prompt, callbacks=[handler])\n",
    "            except Exception as e:\n",
    "                response = f\"Error: {str(e)}\"\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            logs_df.append(\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"run_id\": i + 1,\n",
    "                    \"final_answer\": response,\n",
    "                    \"duration_sec\": round(duration, 3),\n",
    "                    \"reasoning_trace\": \"\\n\".join(handler.logs),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(logs_df)\n",
    "    df.to_csv(filename + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a9de191",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsAnalyse = [\n",
    "    \"What is the average loading delay per supplier?\",\n",
    "    \"Which docks are most frequently blocked and what is the impact on loading schedules?\",\n",
    "    \"What is the distribution of order statuses over the last 30 days?\",\n",
    "    \"Which suppliers handle the largest volume of transport orders?\",\n",
    "    \"How many orders are assigned to the El Kseur docks compared to other regions?\",\n",
    "    \"What percentage of docks handle palletized goods compared to big bags?\",\n",
    "    \"What are the 5 most transported products by quantity?\",\n",
    "    \"How many orders were handled by third-party agencies compared to internal transport?\",\n",
    "    \"Which customer generated the most transport orders this quarter?\",\n",
    "    \"What are the most frequent differences between the planned loading date and the actual date?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e031d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Which supplier has the most orders?\",\n",
    "    \"Give me a summary of the order statuses.\",\n",
    "    \"Which type of product has the most orders?\",\n",
    "    \"What is the most frequent order destination?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90fc26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenging_prompt = \"\"\"\n",
    "Hey there!  \n",
    "\n",
    "I need your help analyzing our transport and dock operations. We’ve been having some delays and inefficiencies, and I’d like you to dig into the data to find out what’s going on. Here’s what I need:  \n",
    "\n",
    "1. Dock Bottlenecks\n",
    "   - Which docks are blocked most often? Are certain types of docks (like those handling big bags or pallets) more likely to be blocked?  \n",
    "   - Do blocked docks cause delays in order completion?\n",
    "\n",
    "2. Vender Performance Check  \n",
    "   - Which vendors complete orders fastest? Show me the top and bottom 3 vendors.  \n",
    "   - Do vendors using external agencies perform worse than those handling transport themselves?  \n",
    "\n",
    "3. Mismatch Alerts  \n",
    "   - Are we sending the wrong types of goods to docks?  How often does this happen, and does it cause issues?  \n",
    "\n",
    "4. El Kseur Delays  \n",
    "   - Orders going through El Kseur docks seem slower. Is this because of distance, vendor assignments, or something else?  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_agent.run(challenging_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85755bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_agent.run(challenging_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_agent(prompts, \"custom_rag_agent\", custom_agent, 2)\n",
    "benchmark_agent(prompts, \"default_agent\", default_agent, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a6195a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_df = pd.read_csv(\"custom_rag_agent.csv\")\n",
    "default_df = pd.read_csv(\"default_agent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec3eb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(260.05899999999997), np.float64(130.00900000000001))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_df[\"duration_sec\"].sum(),default_df [\"duration_sec\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a03313",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
