{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4f0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from typing import Callable, List, Dict, Any\n",
    "from langsmith import traceable, Client\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0cd9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "langsmith_client = Client()\n",
    "\n",
    "# def generate_agent_answers(\n",
    "#     invoke_callback: Callable[[str], str], dataset: List[Dict[str, Any]]\n",
    "# ) -> DataFrame:\n",
    "#     filename = f\"run_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "#     total = len(dataset)\n",
    "#     print(f\"ðŸ” Starting answer generation on {total} dataset entries...\\n\")\n",
    "#     results = []\n",
    "\n",
    "#     for i, entry in enumerate(dataset, start=1):\n",
    "#         run_id = str(uuid.uuid4())\n",
    "#         print(f\"âž¡ï¸  [{i}/{total}] Running entry with name: {run_id}\")\n",
    "\n",
    "#         @traceable(name=run_id, project_name=\"PFE\")\n",
    "#         def orchestrate(input: str) -> str:\n",
    "#             return invoke_callback(input)\n",
    "\n",
    "#         input: str = entry[\"question\"]\n",
    "#         reference_answer: str = entry[\"answer\"]\n",
    "\n",
    "#         generated_output: str = orchestrate(input=input)\n",
    "#         results.append(\n",
    "#             {\n",
    "#                 \"run_id\": run_id,\n",
    "#                 \"input\": input,\n",
    "#                 \"output\": generated_output,\n",
    "#                 \"reference_answer\": reference_answer,\n",
    "#             }\n",
    "#         )\n",
    "#         pd.DataFrame(results).to_csv(filename, index=False)\n",
    "#     return results\n",
    "    \n",
    "def benchmark_agent(results: List[Dict[str, Any]]):\n",
    "    run_ids: List[str] = [i[\"run_id\"] for i in results]\n",
    "    print(f\"ðŸ“¡ Fetching LangSmith run data for {len(run_ids)} runs...\\n\")\n",
    "    runs=list(langsmith_client.list_runs(project_name=\"PFE\", limit=120, execution_order=1))\n",
    "    data: List[Dict[str, Any]] = []\n",
    "    for run in runs:\n",
    "        if run.name not in run_ids:\n",
    "            continue\n",
    "        duration = (\n",
    "            (run.end_time - run.start_time).total_seconds()\n",
    "            if run.end_time and run.start_time\n",
    "            else None\n",
    "        )\n",
    "        d: Dict[str, Any] = {\n",
    "            \"run_id\": run.id,\n",
    "            \"duration (sec)\": duration,\n",
    "            \"input\": run.inputs[\"input\"],\n",
    "            \"output\": run.outputs[\"output\"],\n",
    "            \"error\": run.error,\n",
    "            \"total_tokens\": run.total_tokens,\n",
    "        }\n",
    "        data.append(d)\n",
    "\n",
    "    print(f\"ðŸ“Š Assembling results into DataFrame...\\n\")\n",
    "    df: DataFrame = pd.DataFrame(data)\n",
    "\n",
    "    print(\"âœ… Benchmarking complete.\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fcee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke(query: str) -> str:\n",
    "\n",
    "    from agents.orchestrator import LLMOrchestrator\n",
    "\n",
    "    pandas_agent=LLMOrchestrator()\n",
    "    return pandas_agent.orchestrate(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14ba7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../final_dataset.csv\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d1b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def generate_agent_answers(invoke_callback: Callable[[str], str], dataset: List[Dict[str, Any]]) -> DataFrame:\n",
    "    filename = f\"run_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    total = len(dataset)\n",
    "    print(f\"ðŸ” Starting threaded answer generation on {total} dataset entries...\\n\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    def process_entry(entry_idx: int, entry: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        run_id = str(uuid.uuid4())\n",
    "        print(f\"âž¡ï¸  [{entry_idx+1}/{total}] Running entry with name: {run_id}\")\n",
    "        \n",
    "        @traceable(name=run_id, project_name=\"PFE\")\n",
    "        def orchestrate(input: str) -> str:\n",
    "            return invoke_callback(input)\n",
    "\n",
    "        input_str = entry[\"question\"]\n",
    "        reference_answer = entry[\"answer\"]\n",
    "        output = orchestrate(input=input_str)\n",
    "\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"input\": input_str,\n",
    "            \"output\": output,\n",
    "            \"reference_answer\": reference_answer,\n",
    "        }\n",
    "\n",
    "    # Use ThreadPoolExecutor to parallelize execution\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        # Submit all jobs\n",
    "        futures = [executor.submit(process_entry, i, entry) for i, entry in enumerate(dataset)]\n",
    "\n",
    "        # Gather results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    # Write once all threads are done\n",
    "    pd.DataFrame(results).to_csv(filename, index=False)\n",
    "    print(f\"âœ… Finished all threads. Results saved to {filename}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ebdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = generate_agent_answers(dataset=dataset, invoke_callback=invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd08516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Fetching LangSmith run data for 2 runs...\n",
      "\n",
      "ðŸ“Š Assembling results into DataFrame...\n",
      "\n",
      "âœ… Benchmarking complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df=benchmark_agent(results=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_test_results(df: DataFrame) -> None:\n",
    "    print(\"ðŸ“Š Summary of Test Results:\\n\")\n",
    "    print(\"Total Runs:\", len(df))\n",
    "    print(\n",
    "        \"Successful Runs:\",\n",
    "        df[\"error\"].fillna(\"No Error\").value_counts().get(\"No Error\", 0),\n",
    "    )\n",
    "    print(\"Average Duration:\", df[\"duration (sec)\"].mean(), \"seconds\")\n",
    "    print(\"Total Tokens Used:\", df[\"total_tokens\"].sum(), \"tokens\")\n",
    "    print(\n",
    "        \"Accuracy:\", df[\"verdict\"].value_counts(normalize=True).get(True, 0) * 100, \"%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45eedc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Summary of Test Results:\n",
      "\n",
      "Total Runs: 2\n",
      "Successful Runs: 2\n",
      "Average Duration: 13.1618325 seconds\n",
      "Total Tokens Used: 6722 tokens\n"
     ]
    }
   ],
   "source": [
    "summarize_test_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
