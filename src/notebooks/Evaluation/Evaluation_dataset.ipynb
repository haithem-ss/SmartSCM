{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a842090",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Evaluation Dataset Generation\n",
    "\n",
    "> **Important Note**: This notebook has been sanitized for open-source publication. Original evaluation questions containing company-specific data have been removed.\n",
    "\n",
    "## ðŸ“š Documentation Available\n",
    "\n",
    "Instead of embedded code, please refer to:\n",
    "\n",
    "1. **[EVALUATION_METHODOLOGY.md](./EVALUATION_METHODOLOGY.md)** - Complete evaluation framework and methodology\n",
    "2. **[ORIGINAL_DATASET_ARCHIVED.md](./ORIGINAL_DATASET_ARCHIVED.md)** - Information about the original dataset\n",
    "\n",
    "## ðŸŽ¯ What This Notebook Originally Did\n",
    "\n",
    "This notebook was used to systematically create evaluation questions that tested:\n",
    "- Natural language understanding\n",
    "- Data analysis accuracy  \n",
    "- Query complexity handling\n",
    "- System limitation recognition\n",
    "\n",
    "The methodology and learnings are preserved in the documentation above, while sensitive company data has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Dataset Generation Notebook\n",
    "# ==========================================\n",
    "# This notebook was used to systematically generate evaluation questions\n",
    "# for testing the SmartSCM Assistant's query understanding and data analysis capabilities.\n",
    "# \n",
    "# For detailed methodology, see: EVALUATION_METHODOLOGY.md\n",
    "#\n",
    "# Note: This notebook has been sanitized. Original evaluation used company data\n",
    "# which has been replaced with synthetic datasets.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e0849",
   "metadata": {},
   "source": [
    "# Evaluation Question Categories\n",
    "\n",
    "This notebook originally contained 99 evaluation questions across three difficulty levels:\n",
    "- **33 Easy Questions**: Simple queries testing basic filtering and aggregation\n",
    "- **33 Medium Questions**: Multi-condition queries with grouping and calculations  \n",
    "- **33 Hard Questions**: Complex multi-step analysis with advanced pandas operations\n",
    "- **20+ Out-of-Scope Questions**: Testing the system's ability to recognize limitations\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The evaluation dataset was created following a systematic approach:\n",
    "\n",
    "1. **Schema Analysis**: Reviewed all dataset columns and relationships\n",
    "2. **Pattern Identification**: Identified common analytical query patterns\n",
    "3. **Complexity Progression**: Questions ranged from simple counts to complex statistical analysis\n",
    "4. **Natural Language Variation**: Tested different phrasings and question formats\n",
    "\n",
    "## Sample Question Categories\n",
    "\n",
    "### Easy Level Examples\n",
    "- Count operations: \"How many orders do we have?\"\n",
    "- Simple filters: \"Show me orders by customer\"\n",
    "- Basic aggregations: \"What is the total revenue?\"\n",
    "\n",
    "### Medium Level Examples  \n",
    "- Conditional filtering: \"Which customers have more than 100 orders?\"\n",
    "- Date-based analysis: \"How many orders were placed in January?\"\n",
    "- Grouped calculations: \"Average order value by region\"\n",
    "\n",
    "### Hard Level Examples\n",
    "- Multi-step analysis: \"For each customer, average quantity per product\"\n",
    "- Statistical queries: \"Regions with >50% express shipping\"\n",
    "- Complex conditions: \"Orders with more than 5 different products\"\n",
    "\n",
    "### Out-of-Scope Examples\n",
    "- Missing data: \"How many cancelled orders?\" (no cancellation field)\n",
    "- Wrong timeframe: \"Show orders from 2018\" (data starts 2024)\n",
    "- Non-existent values: Questions about data not in the dataset\n",
    "\n",
    "---\n",
    "\n",
    "**For complete methodology and insights, see [`EVALUATION_METHODOLOGY.md`](./EVALUATION_METHODOLOGY.md)**\n",
    "\n",
    "**Note**: Original questions referenced company-specific data and have been removed to protect confidentiality. The evaluation framework and methodology remain documented for educational purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
